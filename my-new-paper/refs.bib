@book{chomsky1957,
    author = {Chomsky, Noam},
  title = {Syntactic Structures},
  year = {1957},
  publisher = {Mouton}
}

@article{vaswani2017,
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
  title = {Attention Is All You Need},
  journal = {NeurIPS},
  year = {2017}
}

@article{
  hasson2022, 
  author = {A. Goldstein and Z. Zada and E. Buchnik and M. Schain and
    A. Price and B. Aubrey and S. Nastase and S. Feder and
    D. Emanuel and A. Cohen and A. Jansen and H. Gazula and
    G. Choe and A. Rao and C. Kim and C. Casto and L. Fanda and
    W. Doyle and D. Friedman and P. Dugan and L. Melloni and
    R. Reichart and S. Devore and A. Flinker and L. Hasenfratz and
    O. Levy and M. Hassidim and M. Brenner and Y. Matias and
    K. Norman and U. Hasson},
  title = {Shared computational principles for language processing in humans and deep language models},
  journal = { Nat Neurosci.}, 
  year = {2022}
}
@misc{illusion-of-thinking,
title = {The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity},
author = {Parshin Shojaee*† and Iman Mirzadeh* and Keivan Alizadeh and Maxwell Horton and Samy Bengio and Mehrdad Farajtabar},
year = {2025},
URL = {https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf},
style = {association-for-computing-machinery}
}
@inproceedings{lin-etal-2023-solving,
    title = "Solving Linguistic Olympiad Problems with Tree-of-Thought Prompting",
    author = "Lin, Zheng-Lin  and
      Yen, Chiao-Han  and
      Xu, Jia-Cheng  and
      Watty, Deborah  and
      Hsieh, Shu-Kai",
    editor = "Wu, Jheng-Long  and
      Su, Ming-Hsiang",
    booktitle = "Proceedings of the 35th Conference on Computational Linguistics and Speech Processing (ROCLING 2023)",
    month = oct,
    year = "2023",
    address = "Taipei City, Taiwan",
    publisher = "The Association for Computational Linguistics and Chinese Language Processing (ACLCLP)",
    url = "https://aclanthology.org/2023.rocling-1.33/",
    pages = "262--269"
}
@misc{bean2024lingolybenchmarkolympiadlevellinguistic,
      title={LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low-Resource and Extinct Languages}, 
      author={Andrew M. Bean and Simi Hellsten and Harry Mayne and Jabez Magomere and Ethan A. Chi and Ryan Chi and Scott A. Hale and Hannah Rose Kirk},
      year={2024},
      eprint={2406.06196},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.06196}, 
}
@inproceedings{sahin-etal-2020-puzzling,
    title = "{P}uzz{L}ing {M}achines: {A} {C}hallenge on {L}earning {F}rom {S}mall {D}ata",
    author = {{\c{S}}ahin, G{\"o}zde G{\"u}l  and
      Kementchedjhieva, Yova  and
      Rust, Phillip  and
      Gurevych, Iryna},
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.115/",
    doi = "10.18653/v1/2020.acl-main.115",
    pages = "1241--1254",
    abstract = "Deep neural models have repeatedly proved excellent at memorizing surface patterns from large datasets for various ML and NLP benchmarks. They struggle to achieve human-like thinking, however, because they lack the skill of iterative reasoning upon knowledge. To expose this problem in a new light, we introduce a challenge on learning from small data, PuzzLing Machines, which consists of Rosetta Stone puzzles from Linguistic Olympiads for high school students. These puzzles are carefully designed to contain only the minimal amount of parallel text necessary to deduce the form of unseen expressions. Solving them does not require external information (e.g., knowledge bases, visual signals) or linguistic expertise, but meta-linguistic awareness and deductive skills. Our challenge contains around 100 puzzles covering a wide range of linguistic phenomena from 81 languages. We show that both simple statistical algorithms and state-of-the-art deep neural models perform inadequately on this challenge, as expected. We hope that this benchmark, available at \url{https://ukplab.github.io/PuzzLing-Machines/}, inspires further efforts towards a new paradigm in NLP{---}one that is grounded in human-like reasoning and understanding."
}